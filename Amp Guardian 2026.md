Amp-Lite: 2026 Comprehensive Enhancement Plan
Introduction

Amp-Lite is a multi-agent AI coding assistant designed to work in the developer‚Äôs command-line environment (CLI). It integrates multiple specialized AI agents to help plan projects, generate code, write tests, review changes, and more, all while fitting into existing developer workflows. Amp-Lite emphasizes an open, extensible design (in contrast to closed-source alternatives) and uses a structured Product Requirement Prompt (PRP) methodology to turn feature requirements into a step-by-step plan for the agents to execute. This systematic approach ensures the AI works through tasks in a logical order, much like a human developer following a project plan.

This document outlines two critical new additions that will be implemented in Amp-Lite during 2026 to significantly enhance its capabilities and user experience:

Guardian Agent ‚Äì ‚ÄúWorkflow Supervisor‚Äù: an always-on agent that monitors the development workflow and provides timely guidance or reminders (a unique feature not found in other AI coding tools).

RLM Integration ‚Äì ‚ÄúInfinite Context‚Äù: integration of Recursive Language Model techniques to allow Amp-Lite to handle entire codebases or large documents beyond the normal context window of an AI model.

By incorporating these features, Amp-Lite will differentiate itself from existing solutions (like Sourcegraph‚Äôs Amp) with proactive workflow guidance and virtually unlimited context handling, ensuring a smoother and more powerful developer experience. The following sections detail the design and implementation plan for each addition, along with the updated system architecture, development roadmap, and how these changes position Amp-Lite ahead of the competition.

Guardian Agent: Always-On Workflow Supervisor

Purpose: The Guardian Agent is a background AI agent that continuously watches over the development session and catches what the human might forget. Its role is to be a ‚Äúresponsible friend‚Äù or workflow supervisor that keeps the project on track. It watches for potential mistakes, oversights, or deviations from the plan, and gently nudges the user back towards best practices.

Unique Value: This is a novel feature ‚Äî no other coding agent currently offers an always-active workflow supervisor. In traditional AI coding assistants, the AI responds only when asked. In Amp-Lite, the Guardian Agent will be always listening in the background for certain triggers. It won‚Äôt write code, but it will significantly improve the development flow by ensuring important steps aren‚Äôt skipped (like committing code, writing tests, fixing errors, following the PR plan, etc.). This helps prevent issues before they grow (for example, avoiding a huge uncommitted diff or forgotten tests until the end) and gives Amp-Lite users a safety net that improves reliability and efficiency.

Implementation Details: The Guardian Agent will be implemented as a hook that runs on every user prompt submission, maintaining its own state about the project‚Äôs progress. Key aspects of its design include:

State Tracking: The agent keeps track of various pieces of context in order to detect when something is off. For example, it will track: the current PRP step (if the user is following a Product Requirement Prompt plan), how many files have been changed since the last git commit, how long it‚Äôs been since the last commit, whether new code has corresponding new tests, the percentage of the AI‚Äôs context window in use (to warn about context limit), and any known errors or warnings (e.g., compiler or linter errors that haven‚Äôt been resolved). This ‚Äúworkflow state‚Äù is stored in a structure accessible to the Guardian Agent on each prompt. A new workflow_state component will be added to Amp-Lite‚Äôs infrastructure to persist this data during the session.

Proactive Trigger Conditions: Using the state data, the Guardian Agent evaluates a set of trigger conditions after each user action. When a condition is met, it formulates a brief suggestion to the user. Some core trigger categories and examples include:

üìç Commit Nudges: If too many changes are accumulating without a commit, the agent will remind the user to checkpoint their work. For instance: if more than 5 files have changed and it‚Äôs been over ~15 minutes since the last commit, the agent might suggest: ‚Äúüí° You‚Äôve made significant changes (X files) without a commit. Want to checkpoint with a commit?‚Äù Similarly, once a feature seems completed (code for a feature is done) but the user hasn‚Äôt opened a pull request, it could prompt: ‚Äúüí° This feature looks done. Do you want to create a PR for it?‚Äù

üìç Test Nudges: If the user writes new functions or modules but hasn‚Äôt written any tests for them, the Guardian can gently remind them about testing. For example, after detecting new code added with zero new tests, it might say: ‚Äúüí° I see new functions implemented, but no tests yet. Generate some tests before moving on?‚Äù This keeps testing at the forefront so that quality isn‚Äôt deferred to the very end.

üìç Progress Nudges: If the project is following a PRP-defined plan, the agent ensures the user doesn‚Äôt stray too far off course without noticing. For instance, ‚Äúüí° We‚Äôre on step 3 of 7 in the PR plan. The last request looks unrelated ‚Äì would you like to pivot away from the plan, or continue with the next planned step?‚Äù This reminds the user of their current progress and checks if a context switch is intentional. Another progress-related nudge is watching the session length and context size: if the model‚Äôs context window usage exceeds, say, 80%, the agent can warn: ‚Äúüí° The session context is getting full (~80% used). Consider using /handoff to save state before the buffer resets.‚Äù This ties in with Amp-Lite‚Äôs handoff mechanism to prevent context loss.

üìç Error Nudges: When errors or warnings are present, the agent will encourage addressing them promptly. For example, if there‚Äôs a TypeScript compilation error visible in the console and the user proceeds to another task without fixing it, the agent can interject: ‚Äúüí° There‚Äôs still a TypeScript error in auth.ts. Fix that before continuing?‚Äù Likewise, if the user is about to commit code while there are known linter or formatting issues, it might say: ‚Äúüí° Some linting errors are detected. Run the formatter or linter before committing?‚Äù This helps maintain code quality and prevents context-switching away from unresolved problems.

Suggestion Style: All Guardian Agent messages are short, polite, and non-intrusive. They will be prefixed with a lightbulb emoji (üí°) to clearly mark them as tips. The messaging is kept concise (one line if possible) and phrased as a question or gentle reminder, not as a command. This way, the user doesn‚Äôt feel interrupted or patronized, and the suggestions blend into the CLI output in a helpful way. For example:
‚Äúüí° 12 files changed, no commit in 20 minutes. Commit now?‚Äù
‚Äúüí° Step 3/7 in plan. New query seems off-track ‚Äì pivot or continue?‚Äù
‚Äúüí° Auth feature implemented. Open a pull request?‚Äù

Non-Blocking Behavior: Importantly, the Guardian Agent will never block execution or force an action. The user remains in full control. The agent‚Äôs suggestions are just that ‚Äì suggestions. The user can choose to follow the advice or simply ignore it and continue. The system will likely allow an easy way to dismiss or silence certain nudges (for example, the user might type ‚Äú/ignore‚Äù or a similar command if they want to temporarily disable suggestions).

Throttling and Cooldown: To avoid the agent becoming annoying, it will issue at most one nudge per user prompt (no spamming multiple tips at once). If a particular suggestion has been made and the user ignored it, the agent will wait a while before repeating it. For instance, it won‚Äôt remind about committing code again for the next few prompts (or next X minutes) after the first nudge, unless the situation changes (more files added, etc.). This ‚Äúcooldown‚Äù ensures the agent isn‚Äôt nagging the user with the same message repeatedly. The design might enforce something like ‚Äúdon‚Äôt repeat an identical nudge within 5 user messages or 10 minutes‚Äù for example.

Technical Implementation: The Guardian Agent will run as a lightweight sub-agent every time the user enters a prompt (hook: UserPromptSubmit). Under the hood, it can be powered by a smaller, fast model (for example, an Anthropic Claude Instant or OpenAI GPT-3.5 Turbo, or a specialized cheap model) since the tasks are relatively simple and pattern-based. This keeps it cost-effective to have running constantly. The agent will utilize available tools and APIs to gather its needed data; for example, it might call git status (via a CLI tool interface) to see how many files have changed, or use an internal representation of the PRP steps to know the current step, or query a test coverage tool to see if new tests were created. All this logic will be encoded in its prompt or programmatically handled via the tools. The guardian-check hook in Amp-Lite‚Äôs configuration will ensure this agent executes on each cycle.

By implementing the Guardian Agent, Amp-Lite introduces a powerful real-time mentoring feature to the coding workflow. Developers using Amp-Lite will gain an ever-vigilant assistant that helps catch oversight and keep progress on track, without having to remember to ask for it. This addition is expected to significantly improve the user experience, making Amp-Lite sessions more productive and error-free compared to using other AI coding assistants that lack such oversight.

RLM Integration: Unlocking ‚ÄúInfinite‚Äù Context

Why Infinite Context? Large Language Models have an inherent limitation: a fixed-size context window (the amount of text they can consider at once). Even cutting-edge models with extended contexts (like Anthropic‚Äôs 100k-token context model or Sourcegraph‚Äôs Amp with ~400k tokens) eventually hit a limit, and performance tends to degrade as the context grows very large due to a phenomenon often called context rot. In practical terms, this means if you have a massive codebase or a long project history, the AI can‚Äôt simply ‚Äúread‚Äù it all at once without losing some understanding or paying exorbitant costs. Amp-Lite‚Äôs goal is to handle enterprise-scale projects (potentially millions of lines of code or lengthy requirement documents), so a different strategy is needed beyond the standard context window.

What are Recursive Language Models (RLMs)? A Recursive Language Model (RLM) approach is a new paradigm (pioneered in late 2025 by MIT and others) that flips the script on context handling. Instead of dumping all relevant text into the prompt, RLMs treat the large text as an external environment or database that the model can query and navigate programmatically. In an RLM system, the full input (e.g., an entire code repository or a lengthy document) is loaded outside the model ‚Äì for example, stored as a string in a Python environment. The model is then given tools and instructions to fetch pieces of that text as needed, rather than seeing everything at once. Essentially, the model learns to act like a smart reader: it can look up specific parts of the text, break the problem into sub-tasks, call itself on those sub-parts, and gradually build an answer. This approach allows the AI to handle massive contexts piecewise while staying within a small prompt window at any given time.

Benefits of the RLM Approach: Adopting RLM techniques in Amp-Lite will provide several significant advantages:

Near-Unlimited Context: RLM systems can handle inputs orders of magnitude larger than a normal LLM‚Äôs attention window. Research has shown that RLM-based methods can work with inputs about 100√ó larger than the base model‚Äôs context limit, encompassing things like entire codebases, multi-year document archives, or book-length texts. In practice, this means Amp-Lite could analyze all of a company‚Äôs repositories or read an entire design spec book without running out of context.

Improved Reasoning Quality: Interestingly, giving the model the ability to actively choose what to read and when can lead to better results even on smaller tasks. The recursive strategy of focusing on one piece at a time and synthesizing results often outperforms the base model working from one giant prompt, even in cases where the prompt would have fit in context. In other words, RLM isn‚Äôt just a scalability hack; it can actually make the AI‚Äôs reasoning more accurate and logical by breaking complex problems into manageable chunks.

No Information Loss (No Summarization Needed): Unlike strategies that summarize or truncate context to make it fit (which risk losing details), RLMs never permanently throw away information. The model can always go back and fetch more details from the source as needed. As noted by researchers, an RLM-enabled model doesn‚Äôt summarize the context (which can cause important info to be omitted); instead it proactively delegates the task of retrieving relevant information to code or sub-models. Amp-Lite with RLM will, for example, never have to say ‚ÄúI don‚Äôt have earlier parts of the conversation‚Äù ‚Äì because it can always pull them from its external store on demand.

Cost-Effective Scaling: Using RLM techniques can also be more efficient with API usage. Rather than feeding a 500k token prompt (and paying for all those tokens whether they‚Äôre needed or not), the model only processes the fragments of text that are actually relevant to the query. This means the cost for a given task stays in line with a normal prompt, even if the underlying data is huge. In fact, experiments have shown that an RLM can sometimes be cheaper ‚Äì you don‚Äôt necessarily pay 10√ó the cost for a 10√ó larger input, because the model might only end up reading a small portion of that input that matters to the question.

With these benefits in mind, RLM integration will be a game-changer for Amp-Lite, enabling it to scale to enterprise codebases and long-horizon tasks that others struggle with. Here‚Äôs how we plan to integrate RLM capabilities into Amp-Lite:

How RLM Integration Works in Amp-Lite: Amp-Lite will incorporate RLM principles via a new component (the rlm-adapter agent) and supporting infrastructure, replacing or augmenting the old context management system (handoff). The integration will involve several parts:

External Context Store: Instead of injecting the entire codebase or all project files into the prompt (which is impossible for large projects), Amp-Lite will load the codebase into an external storage accessible during the session. Practically, this could be done by reading all repository files into a structured Python object (e.g., a dictionary of file paths to content) or a database. The key is that this data lives outside the LLM‚Äôs direct context. For example, when a session starts, Amp-Lite might use a Python REPL tool to store the project‚Äôs files in a variable (say codebase_text). This makes the full code available to query without blowing up the token count.

System Prompt Instructions: The root agent (the main LLM that orchestrates tasks) will receive a special system prompt at the start of the session explaining the RLM setup and what tools are at its disposal. As described in the MarkTechPost summary of RLM, the model is told that it has access to functions/commands to read slices of the external text, search for keywords, and even spawn other model instances. For example, the prompt might say (paraphrasing): ‚ÄúYou have access to a Python environment with a variable codebase_text containing all the code. You can run Python code to search or slice this text. You can call a function llm_query(text_chunk, question) to spawn a sub-model that answers a question on a specific chunk of text.‚Äù These instructions essentially teach the model how to use the external context effectively rather than naively trying to ingest it all.

Recursive Sub-Agents: When a complex query or task arises, the root model can call sub-models (sub-agents) on portions of the data. For example, suppose the PRP has a step ‚ÄúAudit all authentication-related code for security issues.‚Äù With RLM, the root agent can do the following: 1) run a search in the codebase for ‚Äúauth‚Äù or known auth modules (using a provided tool in the Python REPL) to identify relevant files; 2) for each relevant file, spawn a sub-agent (a smaller LLM instance) that reads that file‚Äôs content (or a part of it) and reports any potential issues or summaries; 3) aggregate those findings to produce a comprehensive report or code changes. The user will only see the final result, but under the hood Amp-Lite might have made dozens of micro-calls to cover every file. This recursive decomposition allows tackling big tasks by breaking them into many small LLM calls ‚Äì a strategy proven to handle long contexts and improve accuracy. Amp-Lite‚Äôs rlm-adapter agent will coordinate these sub-calls.

No Raw Dump in Prompt: The architecture ensures the root LLM never sees giant context blobs. At any given time, the prompt the model is working with will contain at most a summary of what it‚Äôs doing and maybe a snippet of code or text it just fetched, plus the general instructions. For instance, instead of trying to fit 500 files into the prompt, the model might see something like: ‚ÄúYou have a tool to read files. The user asked about X. So far you have checked files A, B, C. File D (snippet) contains Y‚Ä¶ [and so on].‚Äù This way, the prompt stays within manageable size and the model‚Äôs attention is focused. Amp-Lite essentially becomes an open-book exam for the AI: all information is available, but it has to choose what to open and read at each step.

Tooling and Environment: To enable the above, we will integrate a sandboxed Python environment (or similar) for the model to use. The rlm-adapter will likely utilize a library or runtime that supports executing model-written code safely (e.g., whitelisted functions for searching text, splitting strings, etc.). For security and control, Amp-Lite will restrict what this environment can do ‚Äì primarily text processing, no unrestricted system calls unless explicitly allowed. The Prime Intellect implementation of RLM, for example, only allows certain tools in the main loop and pushes anything heavy (like using internet or expensive tools) to the sub-LLMs. Amp-Lite can adopt a similar approach, ensuring that using tools (which might produce large outputs) doesn‚Äôt flood the main model‚Äôs context but is handled in isolated sub-calls.

Integration Points in Workflow: We will apply RLM-based context management in several parts of Amp-Lite‚Äôs workflow:

When generating a PRP (product requirement prompt) from a broad prompt or specification, the system can scan the entire codebase or documentation to gather relevant info before formulating the plan. Instead of missing global context, it can pull in details from anywhere in the repository.

During agent tasks like coding or testing, if the agent needs to understand how a new piece fits in, it can retrieve related modules or usage examples from the codebase on the fly (e.g., ‚Äúsearch the codebase for how function X is used elsewhere‚Äù and bring that info into context).

The /team command (parallel agents) can be enhanced by giving each sub-agent a focused view of the context via RLM. For instance, if multiple agents work on different microservices in parallel, each agent can query just its service‚Äôs files from the global store without interference.

Memory and history: Instead of pushing the conversation history or past learning into the prompt (which might eventually be cut off), we can store the entire session transcripts in the external context and equip the model with a tool to search it. This way, even if a session extends beyond the typical memory window, the model can recall details from earlier by querying, say, the conversation_history variable.

Evolution of Handoff Mechanism: Amp-Lite currently has a ‚Äúhandoff‚Äù system to deal with context overflow (where it saves state when the conversation gets too long). With RLM, the need for summarizing and restarting is greatly reduced. The new approach will be: when the context is nearing capacity, trigger the rlm-context hook to transfer as much as possible into the external store (if not already there) and rely on querying that store. The Guardian Agent will still warn the user (‚Äúcontext 80% full‚Äù) but the action taken will be to activate the RLM mode rather than just dump a summary. Essentially, Amp-Lite will hand off information to the RLM environment continuously, so the conversation can go on without hard resets. This means fewer interruptions and no loss of information ‚Äì a big win for long sessions.

Staying Cutting-Edge: We will implement RLM integration in a way that keeps Amp-Lite flexible for future advances. RLM research is evolving, so the system should be able to adopt improvements (like better strategies for splitting tasks or controlling recursion depth) as they come out. By the time this feature is fully integrated, Amp-Lite will be one of the first developer tools with this ‚Äúinfinite context‚Äù capability built-in. It not only matches what Amp offers (Amp‚Äôs 400k context is large but still finite), but leapfrogs it by removing the hard limit altogether. Amp-Lite users will be able to ask questions or run tasks that involve their entire codebase, with the confidence that the AI can actually utilize all of it. This positions Amp-Lite as a forward-looking platform, ready for the demands of enterprise-scale AI coding assistance in 2026 and beyond.

Updated Architecture with New Components

With the addition of the Guardian Agent and RLM integration, Amp-Lite‚Äôs architecture will expand. Below is an overview of the updated components, including agents, hooks, and infrastructure elements. (‚úÖ indicates already implemented; üî≤ indicates new or to-be-implemented items.)

Agents (10 Total): Amp-Lite is composed of multiple cooperating AI agents, each responsible for a specific aspect of the software development workflow. The new plan increases the count of agents from 8 to 10 with the two additions. Here‚Äôs the complete list:

Agent Name	Status	Role / Purpose
planner-agent	‚úÖ Done	Parses the Product Requirement Prompt (PRP) and breaks down requirements into a task list or plan (steps for implementation).
implementation-agent	‚úÖ Done	Writes code to implement each task/feature as described by the planner (essentially the coding agent).
reflection-agent	‚úÖ Done	Performs self-reflection and improvement; after tasks, it reviews the work and suggests fixes or better approaches (helps the system learn from mistakes).
architect-agent	üî≤ Planned	Handles high-level system design decisions. For larger features, ensures architecture (design patterns, module structure) is sound before implementation.
testing-agent	üî≤ Planned	Generates and runs tests for the new code. Ensures that for every implementation step, corresponding tests are created to validate functionality.
review-agent	üî≤ Planned	Conducts code reviews on the changes. It will check for code quality, style compliance, potential bugs, and improvement suggestions ‚Äì like a human pull request reviewer.
documentation-agent	üî≤ Planned	Creates documentation and relevant comments for the new changes. It can update README files, code comments, or API docs to reflect the new code.
team-coordinator	üî≤ Planned	Orchestrates parallel agent collaboration (for the /team command). This agent splits work among multiple other agents and then merges results ‚Äì enabling parallel execution for speed-up on large tasks.
guardian-agent (New)	üî≤ Planned	Workflow Supervisor. Monitors the user‚Äôs actions and the overall session state to provide on-the-fly guidance (reminders to commit, write tests, follow plan, etc., as detailed above). Always active via a hook.
rlm-adapter (New)	üî≤ Planned	RLM Interface Agent. Manages the interaction between the LLM and the external context store. It handles the logic of reading/writing from the codebase variable, spawning sub-agents for recursion, and feeding results back to the main model. In essence, this agent encapsulates the RLM strategy so other agents can use infinite context seamlessly.

Hooks (12 Total): Hooks are event-driven triggers in the Amp-Lite system that invoke certain logic (often involving one of the agents above or some governance function). The updated list of hooks with the new additions is as follows:

Hook Name	Status	Purpose and Functionality
Damage Control	‚úÖ Active	Monitors for and blocks dangerous commands or outputs (e.g., preventing the AI from executing a destructive shell command). This is a safety layer.
File Protection	‚úÖ Active	Protects critical or non-generated files from being accidentally modified by the AI (unless explicitly allowed). For example, to prevent deletion of .gitignore or configuration files unless intended.
Correction Detection	‚úÖ Active	Detects when the AI‚Äôs output is incorrect or there are errors (for instance, syntax errors in generated code) and flags these for the reflection-agent or for re-planning. Helps the system catch mistakes autonomously.
Reflection Reminder	‚úÖ Active	After certain milestones or at intervals, triggers the reflection-agent to do a self-review. Ensures the system periodically assesses its progress and lessons learned (especially after finishing a task).
Ralph Continuation	üî≤ Planned	Ensures the agent ‚Äúdoesn‚Äôt quit early.‚Äù This hook is inspired by the ‚ÄúRalph loop‚Äù concept ‚Äì it will automatically continue or re-invoke the agent on a task if the task isn‚Äôt complete yet. Essentially, it forces the agent to keep iterating (up to some limit or until success) rather than stopping after a single response. This is crucial for autonomous multi-step task completion.
Memory Injection	üî≤ Planned	On each prompt or agent invocation, this hook will retrieve relevant snippets from the system‚Äôs long-term memory (SQLite DB or vector store) and inject them into the prompt. For example, if the current task is similar to something done last week, it might pull in the notes from last week‚Äôs session. This enables cross-session and long-term knowledge retention.
Handoff Trigger	üî≤ Planned	Watches the conversation length/context size. When it exceeds a threshold, this hook kicks in to preserve the state (e.g., summarizing or storing the conversation so far). In the RLM-enhanced system, this may instead trigger loading context into the RLM environment. It ensures that long sessions do not lose information when the context window overflows.
Auto-Format	üî≤ Planned	After the implementation-agent generates code, this hook can automatically run a formatter (like Prettier for JS/TS, Black for Python, etc.) on the code. It ensures consistent coding style without the user having to request it manually.
Test Runner	üî≤ Planned	When tests are generated (by testing-agent) or when significant code is written, this hook can automatically run the test suite (or a subset of tests) to provide immediate feedback on whether the changes pass the tests. This catches runtime or integration errors early.
Checkpoint	üî≤ Planned	This hook can automatically create git commits at safe points (with user approval or predefined settings). For example, after a PRP step is completed successfully, it might commit the changes with a message. This provides an ‚Äúautosave‚Äù functionality for the codebase, so progress isn‚Äôt lost and changes are nicely compartmentalized.
guardian-check (New)	üî≤ Planned	The hook backing the Guardian Agent. This is triggered on each user input (prompt submission) and invokes the guardian-agent‚Äôs logic to decide if a nudge is needed. Essentially, it pipes the current workflow_state into the guardian-agent and then outputs any suggestion from it.
rlm-context (New)	üî≤ Planned	Manages the RLM context environment. For instance, this hook might run at session start to initialize the codebase loading into the external store, or when a handoff is triggered to ensure all relevant information is in the external memory. It could also clean up or update the context store as files change (keeping the external codebase_text in sync with the latest code edits). This works closely with the rlm-adapter agent.

Infrastructure Components (6 Total): These are foundational pieces that support the agents and hooks, especially for persistence and performance. With the new features, a couple of new infrastructure elements are introduced:

Component	Status	Purpose and Notes
SQLite Memory DB	üî≤ Planned	A persistent lightweight database to store information between sessions and for long-term memory. This can include past prompts, decisions made by the reflection-agent, user preferences, etc. It enables Amp-Lite to remember context across restarts (important for continuous improvement).
Vector Embeddings Store	üî≤ Planned	A semantic memory index (using embeddings). This allows Amp-Lite to perform semantic search on past conversations, documentation, or code. For example, to find relevant snippets by meaning, not just exact keywords. It complements the SQLite DB by enabling quick retrieval of related info when context is injected.
Observability Server	üî≤ Planned	A monitoring dashboard or logging system to observe the internal workings of the agents. This might record events like which agent is running, how long tasks take, token usage per agent, etc., in real-time. It‚Äôs crucial for developers of Amp-Lite to debug and optimize the multi-agent orchestration, and for users to gain insight (especially in an enterprise setting, to trust what the AI is doing).
Cost Tracker	üî≤ Planned	A utility that tracks API usage (tokens, requests) and perhaps enforces budgets. It will help ensure that using Amp-Lite remains cost-effective, by warning if a certain operation (like an RLM recursion) might become too expensive, or by suggesting a cheaper approach. This is especially relevant as we integrate potentially expensive calls (multiple sub-agents, etc.).
RLM Runtime (New)	üî≤ Planned	The execution environment for the RLM mechanism. This includes the sandboxed Python REPL (or similar) that holds the external context (codebase text, etc.) and allows the model to run code. It may be a persistent process that the LLM can interact with throughout the session. The RLM runtime will also manage sub-model calls (possibly using threads or an async queue to handle multiple sub-LLM queries in parallel for efficiency).
Workflow State (New)	üî≤ Planned	A storage mechanism (likely in-memory, with possible persistence to the SQLite DB) for the Guardian Agent‚Äôs state. It keeps the current values of things like files_changed_count, minutes_since_last_commit, current_PRP_step, context_usage_percent, etc. This state is updated whenever relevant events happen (file edit, commit, step completion, error occurrence, etc.) so that the Guardian Agent has up-to-date info on which to base its advice.

By combining these agents, hooks, and infrastructure components, Amp-Lite‚Äôs architecture will be robust and feature-rich. The Guardian Agent (via guardian-check hook and its workflow state tracking) introduces a feedback loop that constantly checks the system‚Äôs own process. The RLM Adapter (via rlm-context hook, RLM runtime, and rlm-adapter agent) provides a scalable memory system to handle large inputs. All the while, the core workflow of planning, coding, testing, reviewing, and documenting is handled by the specialized agents working in concert. The design is modular ‚Äì each agent or hook has a single responsibility ‚Äì which makes the system easier to extend and maintain.

The above components list also serves as a checklist for implementation: it shows which parts are done and which are planned, guiding the development efforts in the coming phases.

Roadmap and Continuous Improvement Plan (2026)

Implementing the features above will be done in stages. The development roadmap for 2026 is divided into phases, ensuring that Amp-Lite reaches a Minimum Viable Product (MVP) quickly and then builds on it with advanced features. In parallel, Amp-Lite will adopt a philosophy of continuous improvement, meaning even after initial implementation, the agents will be refined and updated regularly as we gather feedback and as AI technology evolves.

P0 ‚Äì Must-Have (MVP) ‚Äì Immediate priorities to get a working, differentiated product in early 2026:

Guardian Agent (Workflow Supervisor): This is the top priority to implement first. It‚Äôs Amp-Lite‚Äôs most original feature and will have an immediate positive impact on user experience. The tasks here include coding the guardian-check hook, developing the logic for each nudge condition, setting up the state tracking (and perhaps a simple UI element in CLI to show a nudge), and testing it thoroughly on typical scenarios to fine-tune the sensitivity (to avoid too many or too few suggestions). We want this ready in the MVP because it‚Äôs a key selling point that no competitor has.

Handoff/Context Management: Before full RLM integration is ready, we need a reliable handoff mechanism to handle long conversations. For MVP, this could be a simpler version where if context gets too large, the system automatically summarizes older parts and stores them in the SQLite memory (and can retrieve them on demand). The Guardian Agent‚Äôs context warning nudge ties into this ‚Äì when we hit ~80% usage, it can prompt and then invoke the handoff. This prevents the dreaded ‚ÄúSorry, I forgot what we were doing‚Äù problem in the middle of a session. Even though RLM will eventually supersede this, having basic handoff in MVP is important for usability in long sessions.

‚ÄúRalph‚Äù Continuation Loop: Implement the Ralph Continuation hook to enable autonomous multi-step execution. MVP should demonstrate that Amp-Lite can carry out a PRP (with multiple steps) mostly on its own. For example, if the planner-agent outlines 5 steps, the implementation-agent (and others) should be able to loop through them without the user having to prompt ‚Äúcontinue‚Äù at each step. Setting up this loop (with a limit to avoid infinite loops) will allow Amp-Lite to actually finish tasks it starts. This will differentiate it from simpler single-turn tools and move it closer to full automation. We‚Äôll need to carefully test this to ensure the loop stops when the task is truly done or if it gets stuck (perhaps by integrating with reflection-agent to decide when to break out if progress stalls).

P1 ‚Äì Amp Parity (Core Features Completion) ‚Äì Mid 2026 goals, focusing on features that bring Amp-Lite to parity with Sourcegraph‚Äôs Amp or similar AI coding assistants in terms of standard functionality:

Parallel ‚ÄúTeam‚Äù Mode: Implement the team-coordinator agent and the /team command. This allows the user to spin up multiple agents working together on separate subtasks, which Amp already offers for complex tasks. Achieving this means implementing the communication between team-coordinator and child agents, dividing a problem (possibly based on PRP or user instructions) into parallel threads. For example, in a big refactor, one agent could update code while another updates tests simultaneously. We‚Äôll need to ensure the results are merged coherently. By mid-2026, Amp-Lite should handle parallelism so it‚Äôs not slower or less capable than Amp on large jobs.

Memory Layer & Learning: Finalize the SQLite memory and vector embeddings integration. Amp-Lite should remember user preferences (for example, coding style guidelines, or the fact that the user said ‚Äúdon‚Äôt use a certain library‚Äù in a past session) and relevant past outcomes. This also includes developing the memory injection hook so that whenever a new question or task comes in, the system automatically pulls any similar past Q&A or code from the vector store. Amp likely has some enterprise memory features; Amp-Lite must meet or exceed that by allowing persistent learning. By this stage, the reflection-agent can also start logging ‚Äúlessons learned‚Äù into the memory after each session (e.g., ‚ÄúWe tried X and it didn‚Äôt work, next time avoid X‚Äù), building a knowledge base that grows with use.

Complete Agent Suite: During this phase, all the ‚Äúplanned‚Äù agents should be implemented and functional:

The architect-agent to handle upfront design decisions (preventing the implementation-agent from going down a wrong path early on by having a higher-level check).

The testing-agent to ensure every code change is accompanied by tests, and possibly to run those tests via the test runner hook.

The review-agent to perform automated code review, catching issues or suggesting improvements just like a human reviewer would in a pull request.

The documentation-agent to generate documentation alongside new code, ensuring that the code is not only working but also well-understood by humans.

By completing these, Amp-Lite covers the entire development cycle (plan ‚Üí code ‚Üí test ‚Üí review ‚Üí document) with AI assistance. Amp (the original) already has features like code review agents, so we need to ensure Amp-Lite isn‚Äôt lacking in those departments. Each of these agents will require careful prompt design and perhaps fine-tuning on example scenarios (for instance, feeding it some known code issues and seeing if review-agent spots them, etc.). By end of P1, Amp-Lite should be a fully capable coding co-pilot on par with or better than Amp‚Äôs standard toolkit.

P2 ‚Äì Competitive Advantage (Beyond Parity) ‚Äì Late 2026 and beyond, focusing on the innovative features that set Amp-Lite apart and continuous enhancements:

RLM Integration Rollout: This is a major undertaking that might span mid to late 2026. The goal is to replace the interim handoff mechanism with the full RLM-based infinite context system. In this stage, we implement the rlm-adapter agent and related runtime, then gradually enable features like querying the entire codebase. We will likely do this in steps: first allow read-only queries (so the model can fetch information), then perhaps allow it to write intermediate results or call sub-models. We‚Äôll test on increasingly larger contexts (start with a moderately large repo, then a huge monorepo) to iron out performance issues. The success criteria is that by the end of 2026, Amp-Lite can handle, for example, a codebase with 1 million lines across hundreds of files and answer questions or make changes that consider all of it. This will be a marquee capability for Amp-Lite, putting it ahead of other offerings. We will also keep an eye on emerging RLM frameworks (from projects like Prime Intellect or others) to possibly incorporate their improvements (e.g., if a library for easy RLM comes out, we might integrate that rather than reinvent the wheel).

Skills Library (Reusable Tactics): As Amp-Lite gains experience, we plan to develop a library of reusable skills or prompt templates that agents can use. For example, a template for ‚Äúadding a new CRUD endpoint to a web service‚Äù which involves updates to routes, controller, model, tests, docs ‚Äì a sequence of steps that could be abstracted. Or a skill for ‚Äúmigrate library X to version Y across all projects‚Äù. These would be like high-level recipes that the planner-agent (or team-coordinator) could pull from when it recognizes a common task. This idea is to reduce the time needed to handle common scenarios and capture best practices. Over 2026, we can gather such patterns from user queries and our own usage, building out this library. This becomes a competitive edge because Amp (the original) might not have a curated skill library and relies on the model doing everything from scratch each time.

Observability & Feedback Loop: By late 2026, we want Amp-Lite to have robust observability. We will deploy the observability server and possibly a UI that shows agent operations, timelines, resource usage, etc. Using this, we (and power users) can identify bottlenecks or failure cases in the multi-agent system. More importantly, it feeds into a feedback loop for continuous improvement: for instance, if we see that the Guardian Agent frequently gives a particular nudge and the user always ignores it, that suggests we should tweak that rule or make the agent smarter. Or if the RLM adapter is spawning too many sub-agents and causing slowdowns, we can optimize the strategy (maybe use a bigger model for summarization in those cases or adjust search granularity). Essentially, data-driven iteration will guide refinements beyond the initial implementation. Amp-Lite being open source means the community can also contribute fixes and improvements when they observe something in the logs or dashboard.

Advanced Learning (Self-Improvement): With the foundation in place, we can explore training Amp-Lite‚Äôs agents to become smarter over time. This could involve techniques like reinforcement learning or fine-tuning based on feedback. For example, we could log instances where the user had to correct the AI (maybe the review-agent missed a bug that the user caught in code review). Those instances can become training data for the future or be analyzed by the reflection-agent to update its heuristics. We might integrate something like a reward model where if the user approves a pull request with no comments, the agents involved get positive feedback, whereas if the user heavily edits the output, the agents treat that as a learning opportunity. By the end of 2026, our aim is that Amp-Lite doesn‚Äôt remain static; the more it‚Äôs used, the better it gets at aligning with the user‚Äôs project and preferences. This is a differentiator because many AI coding tools (Amp included, at least currently) do not have an explicit self-improvement loop ‚Äì they rely on the base model‚Äôs training and maybe occasional fine-tunes, but they aren‚Äôt learning on the fly for each user/team.

Community and Open-Source Development: A big part of Amp-Lite‚Äôs 2026 journey will be fostering a community around it. We plan to open-source the project (if not already done in MVP) and encourage contributors to add agents, improve prompts, create new hooks, etc. This can massively accelerate development. For instance, the open-source community might contribute integrations with popular tools (maybe someone writes a new agent for database migrations, or a hook for Slack notifications on agent progress). Being open source is itself a competitive advantage over closed solutions like Amp ‚Äì companies and individual developers can inspect the code, ensure there are no data leaks or hidden behaviors, and modify it to fit their needs. We‚Äôll maintain a roadmap and good documentation to make it easy for others to get involved. By late 2026, we hope to see external contributions making meaningful additions to Amp-Lite, and perhaps even organizations adopting it and sharing back improvements.

Throughout all these phases, we will maintain flexibility to adjust priorities based on user feedback. For example, if early adopters find the Guardian Agent to be extremely helpful (or conversely, too chatty), we will iterate on its behavior quickly. If the RLM integration proves complex, we might release it behind a feature flag or in beta to gather feedback before full rollout. The continuous improvement mindset means Amp-Lite‚Äôs development doesn‚Äôt end at a fixed feature set ‚Äì it‚Äôs an evolving product. By tackling the P0 and P1 items first, we ensure we have a strong foundation, and then P2 items truly push the envelope and keep Amp-Lite ahead of the curve through 2026.

Competitive Edge Over Amp (and Others)

Amp-Lite‚Äôs enhancements are not just about adding features for their own sake ‚Äì they are designed to leapfrog the existing leading solution (Sourcegraph‚Äôs Amp) and carve out a unique identity in the market. The table below summarizes how Amp-Lite (with the 2026 plan implemented) compares to Amp in key areas:

Feature / Capability	Amp (Sourcegraph)	Amp-Lite (2026)
Multi-Agent Orchestration	‚úÖ Yes (agents toolkit)	‚úÖ Yes (planner, coder, tester, etc. fully orchestrated)
Context Handoff / Management	‚úÖ Yes (400k context + some context saving)	‚úÖ Yes (automatic handoff, evolving into RLM infinite context)
Parallel Agent Teams	‚úÖ Yes (parallel flows)	‚úÖ Yes (planned /team for parallelism)
Guardian Agent (Workflow Supervision)	‚ùå No ‚Äì Not available	‚úÖ Yes ‚Äì Unique feature (always-on error prevention and guidance)
Self-Improvement Loop (Reflect & Learn)	‚ùå No explicit loop	‚úÖ Yes ‚Äì Unique (reflection-agent plus continuous learning hooks)
RLM Integration (Infinite Context)	‚ùå No (fixed window only)	‚úÖ Yes ‚Äì Unique (truly unlimited context via RLM)
PRP Methodology (Structured planning)	‚ùå No (ad-hoc prompts)	‚úÖ Yes (Product Requirement Prompts for systematic development)
Open Source Availability	‚ùå No (Closed source SaaS)	‚úÖ Yes (Open source, community-driven)

(‚úÖ = available, ‚ùå = not available in that product)

Two features stand out as Amp-Lite‚Äôs competitive differentiators: the Guardian Agent and RLM Integration. The Guardian Agent directly addresses a gap in current AI coding assistants ‚Äì none of them actively supervise the workflow for mistakes. This means Amp-Lite users get an experience akin to an expert pair-programmer looking over their shoulder, something Amp cannot offer. The RLM-based infinite context means Amp-Lite can utilize information well beyond the input size of any single model. Amp, even with its impressive 400k token window, will struggle with, say, a million-token codebase or a very long conversation. Amp-Lite, by contrast, will be able to navigate and reason over entire repositories or document sets without breaking a sweat. This is a fundamental capability for enterprise use (where context is massive) and will position Amp-Lite as the more scalable solution.

Moreover, Amp-Lite‚Äôs adoption of the PRP methodology and a full suite of development agents means it follows a consistent, repeatable process for coding tasks. This is likely to result in more predictable outcomes than a more free-form approach. Amp-Lite being open source adds trust and extensibility ‚Äì companies can host it internally, customize it, and contribute improvements. In contrast, Amp is a proprietary tool (with a pay-as-you-go model) which some companies might be wary of for confidential code or long-term dependency.

In summary, by the end of 2026, Amp-Lite isn‚Äôt just catching up to Amp; it‚Äôs poised to surpass it in key dimensions. It introduces new ideas to the field (Guardian Agent and RLM integration) that will raise the bar for what developers expect from an AI coding assistant. This unique value proposition will be a strong selling point for Amp-Lite, whether it‚Äôs to individual developers, open-source enthusiasts, or enterprise teams looking for an AI pair programmer that truly integrates with their workflow and scale.

Conclusion and Next Steps

This comprehensive plan lays out the vision for Amp-Lite through 2026, focusing on robustness, intelligence, and innovation. By implementing the Guardian Agent and RLM ‚Äúinfinite context‚Äù features, Amp-Lite will address two of the most significant pain points in AI-assisted development: the lack of proactive guidance and the limitations of context size. Combined with completing the suite of coding agents and a continuous improvement framework, Amp-Lite will evolve into a next-generation developer assistant that is both reliable and cutting-edge.

The immediate next step is to implement the Guardian Agent. This will involve work on multiple fronts: creating the prompt logic for the agent itself (the questions it asks and how it detects conditions), setting up the workflow state tracking, and integrating the hook so it runs smoothly on each prompt without noticeable lag. We will also need to test it in realistic scenarios ‚Äì for example, simulate a user forgetting to commit, or writing code without tests ‚Äì to see that the suggestions trigger correctly and are helpful. Early feedback from such tests will allow us to tweak thresholds and messaging before releasing it to users.

In parallel, groundwork for the RLM integration can begin by setting up the infrastructure: choosing or building the sandboxed environment, deciding how to represent the codebase in memory (plain text vs. AST index, etc.), and perhaps experimenting with small prototypes (like asking the model to find a string in a large text via a tool call). This groundwork will make the full integration smoother when we dive into it as a dedicated project in P2.

Throughout 2026, we will iterate on all features. No plan remains static, especially in a fast-moving field like AI; Amp-Lite will be continuously refined. We will keep an eye on user feedback and emerging research (e.g., improvements in RLM techniques, new AI model releases, etc.) and incorporate them to keep Amp-Lite state-of-the-art. By the end of the year, we expect to have not just built these features, but to have gone through several cycles of improvement on each, making them robust in real-world use.

In conclusion, Amp-Lite‚Äôs development plan is ambitious but achievable, and it directly targets the areas that will make the biggest difference for users. Focusing on helping the user stay on track (Guardian Agent) and giving the AI the ability to handle any amount of information (RLM) will make Amp-Lite an indispensable tool for modern software development. We have a clear path: build the Guardian Agent, reach parity with competitors, then push beyond with RLM and other advanced capabilities. Each step of this journey will bring Amp-Lite closer to its vision of being a tireless, smart, and friendly partner in every coding project. The excitement and momentum are high, and the next task is clear ‚Äì time to start coding the Guardian Agent and turn this plan into reality!